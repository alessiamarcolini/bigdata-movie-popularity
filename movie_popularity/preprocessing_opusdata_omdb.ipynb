{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from secrets import OMDB_API_KEY, HADOOP_USER_NAME, SPARK_URI, HADOOP_NAMENODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HADOOP_USER_NAME'] = HADOOP_USER_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "import pyspark.sql.functions as F\n",
    "from hdfs import InsecureClient\n",
    "import omdb\n",
    "from omdb import OMDBClient\n",
    "import pyspark.sql.types as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_hdfs = InsecureClient(f'http://{HADOOP_NAMENODE}:50070', user=HADOOP_USER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get preprocessed opusdata filename\n",
    "hdfs_path = \"/processed/opusdata.csv\"\n",
    "\n",
    "filename = [f for f in client_hdfs.list(hdfs_path) if f.endswith('.csv')][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/utente/spark-2.4.5-bin-hadoop2.7/./bin/spark-submit', 'pyspark-shell'] {'CONDA_SHLVL': '2', 'LS_COLORS': 'rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:', 'CONDA_EXE': '/home/utente/anaconda3/bin/conda', 'SSH_CONNECTION': '5.90.206.137 19935 10.0.0.8 22', 'LESSCLOSE': '/usr/bin/lesspipe %s %s', 'SPARK_YARN_USER_ENV': 'PYSPARK_PYTHON=/home/utente/anaconda3/envs/bigdata/bin/python', 'LANG': 'C.UTF-8', 'LESS': '-R', 'AMD_ENTRYPOINT': 'vs/server/remoteExtensionHostProcess', 'OLDPWD': '/home/utente/spark-2.4.5-bin-hadoop2.7/sbin', 'COLORTERM': 'truecolor', 'CONDA_PREFIX': '/home/utente/anaconda3/envs/bigdata', 'JAVA_HOME': '/usr', 'ZSH': '/home/utente/.oh-my-zsh', '_CE_M': '', 'APPLICATION_INSIGHTS_NO_DIAGNOSTIC_CHANNEL': 'true', 'XDG_SESSION_ID': '4', 'USER': 'utente', 'PAGER': 'cat', 'LSCOLORS': 'Gxfxcxdxbxegedabagacad', 'CONDA_PREFIX_1': '/home/utente/anaconda3', 'PWD': '/home/utente', 'HOME': '/home/utente', 'CONDA_PYTHON_EXE': '/home/utente/anaconda3/bin/python', 'VSCODE_GIT_ASKPASS_NODE': '/home/utente/.vscode-server/bin/5763d909d5f12fe19f215cbfdd29a91c0fa9208a/node', 'TERM_PROGRAM': 'vscode', 'SSH_CLIENT': '5.90.206.137 19935 22', 'TERM_PROGRAM_VERSION': '1.45.1', 'TMUX': '/tmp/tmux-1000/default,8276,0', 'XDG_DATA_DIRS': '/usr/local/share:/usr/share:/var/lib/snapd/desktop', '_CE_CONDA': '', 'VSCODE_IPC_HOOK_CLI': '/tmp/vscode-ipc-2cc7f37e-438c-485f-a25b-c7a25f171dab.sock', 'SPARK_HOME': '/home/utente/spark-2.4.5-bin-hadoop2.7', 'CONDA_PROMPT_MODIFIER': '(bigdata) ', 'MAIL': '/var/mail/utente', 'VSCODE_GIT_ASKPASS_MAIN': '/home/utente/.vscode-server/bin/5763d909d5f12fe19f215cbfdd29a91c0fa9208a/extensions/git/dist/askpass-main.js', 'TERM': 'xterm-color', 'SHELL': '/bin/bash', 'PYSPARK_DRIVER_PYTHON': '/home/utente/anaconda3/envs/bigdata/bin/python', 'TMUX_PANE': '%0', 'SHLVL': '5', 'VSCODE_GIT_IPC_HANDLE': '/run/user/1000/vscode-git-9fbc49ba3d.sock', 'PIPE_LOGGING': 'true', 'LOGNAME': 'utente', 'GIT_ASKPASS': '/home/utente/.vscode-server/bin/5763d909d5f12fe19f215cbfdd29a91c0fa9208a/extensions/git/dist/askpass.sh', 'XDG_RUNTIME_DIR': '/run/user/1000', 'PYSPARK_PYTHON': '/home/utente/anaconda3/envs/bigdata/bin/python', 'PATH': '/home/utente/.vscode-server/bin/5763d909d5f12fe19f215cbfdd29a91c0fa9208a/bin:/home/utente/anaconda3/envs/bigdata/bin:/home/utente/anaconda3/condabin:/home/utente/.vscode-server/bin/5763d909d5f12fe19f215cbfdd29a91c0fa9208a/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin', 'CONDA_DEFAULT_ENV': 'bigdata', 'LESSOPEN': '| /usr/bin/lesspipe %s', 'VERBOSE_LOGGING': 'true', '_': '/home/utente/anaconda3/envs/bigdata/bin/jupyter', 'KERNEL_LAUNCH_TIMEOUT': '40', 'JPY_PARENT_PID': '8317', 'CLICOLOR': '1', 'GIT_PAGER': 'cat', 'MPLBACKEND': 'module://ipykernel.pylab.backend_inline', 'HADOOP_USER_NAME': 'hadoop', '_PYSPARK_DRIVER_CONN_INFO_PATH': '/tmp/tmp44pig0b0/tmplydoxhcr'}\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(SPARK_URI)\n",
    "sparkSession = (\n",
    "    SparkSession.builder.appName(\"preprocessing-opusdata-and-omdb\")\n",
    "    .config(\"spark.hadoop.dfs.client.use.datanode.hostname\", \"true\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from hdfs\n",
    "opusdata = sparkSession.read.csv(\n",
    "    f\"hdfs://{HADOOP_NAMENODE}:8020{hdfs_path}/{filename}\", header=True, inferSchema=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+-----------------+------+-----------------+------+----------------+-------+\n",
      "|          movie_name|production_year|production_budget|rating|            genre|sequel|total_box_office|success|\n",
      "+--------------------+---------------+-----------------+------+-----------------+------+----------------+-------+\n",
      "|       Runner Runner|           2013|         30000000|     R|Thriller/Suspense|     0|        60512680|      1|\n",
      "|The Hunger Games:...|           2015|        160000000| PG-13|Thriller/Suspense|     1|       648986787|      1|\n",
      "|   Chasing Mavericks|           2012|         20000000|    PG|            Drama|     0|         8300821|      0|\n",
      "|        Darkest Hour|           2017|         30000000| PG-13|            Drama|     0|       150355828|      1|\n",
      "|       Inherent Vice|           2014|         20000000|     R|            Drama|     0|        14772346|      0|\n",
      "|       Live by Night|           2015|         65000000|     R|            Drama|     0|        21774432|      0|\n",
      "|          The Iceman|           2012|         10000000|     R|            Drama|     0|         3623609|      0|\n",
      "|         Warm Bodies|           2012|         30000000| PG-13|  Romantic Comedy|     0|       115121608|      1|\n",
      "|Avengers: Age of ...|           2014|        330600000| PG-13|           Action|     1|      1403013963|      1|\n",
      "|Billy Lynnâ€™s Long...|           2015|         40000000|     R|            Drama|     0|        30230402|      0|\n",
      "|    Edge of Tomorrow|           2011|        178000000| PG-13|           Action|     0|       370541256|      1|\n",
      "|     Horrible Bosses|           2010|         35000000|     R|           Comedy|     0|       212417601|      1|\n",
      "|          Mei Ren Yu|           2016|         60720000|     R|  Romantic Comedy|     0|       554516671|      1|\n",
      "|Sin City: A Dame ...|           2011|         65000000|     R|           Action|     1|        40650842|      0|\n",
      "|    The Great Gatsby|           2012|        190000000| PG-13|            Drama|     0|       351040419|      0|\n",
      "|               Vamps|           2011|         16000000| PG-13|  Romantic Comedy|     0|           94812|      0|\n",
      "| Wrath of the Titans|           2012|        150000000| PG-13|        Adventure|     1|       305270083|      1|\n",
      "|Extremely Loud an...|           2011|         40000000| PG-13|            Drama|     0|        55247881|      0|\n",
      "|   Midnight in Paris|           2010|         30000000| PG-13|  Romantic Comedy|     0|       162502774|      1|\n",
      "|The Man From U.N....|           2015|         75000000| PG-13|           Action|     0|       104949584|      0|\n",
      "+--------------------+---------------+-----------------+------+-----------------+------+----------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "opusdata.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1163"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opusdata.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "omdb.set_default('apikey', OMDB_API_KEY)\n",
    "client = OMDBClient(apikey=OMDB_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "requested_flat_fields = ['runtime', 'director', 'actors', 'country', 'awards', 'imdb_votes', 'imdb_id']\n",
    "requested_nested_fields = {'ratings': ['Internet Movie Database', 'Rotten Tomatoes', 'Metacritic']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_source(source):\n",
    "    return '_'.join(source.split()).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_schema(requested_flat_fields, requested_nested_fields):\n",
    "    schema = []\n",
    "    for key in requested_flat_fields:\n",
    "        schema.append(t.StructField(key, t.StringType(), True))\n",
    "    for key, values in requested_nested_fields.items():\n",
    "        for value in values:\n",
    "            schema.append(t.StructField(f'{key}_{format_source(value)}', t.StringType(), True))\n",
    "            \n",
    "    return t.StructType(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = construct_schema(requested_flat_fields, requested_nested_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=schema)\n",
    "def omdb_data(arguments):\n",
    "\n",
    "    movie_name, year = arguments\n",
    "    print(movie_name)\n",
    "    result = client.get(title=movie_name, year=year, fullplot=True, tomatoes=True)\n",
    "    \n",
    "    result_to_keep = {}\n",
    "    \n",
    "    for key in requested_flat_fields:\n",
    "        result_to_keep[key] = result.get(key, None)\n",
    "        \n",
    "    for nested_field in requested_nested_fields:\n",
    "        requested_nested_list = requested_nested_fields[nested_field]\n",
    "        nested_list = result.get(nested_field, None)\n",
    "        \n",
    "        if nested_list:\n",
    "            for nested_dict in nested_list:\n",
    "                source = nested_dict.get('source', None)\n",
    "\n",
    "                if source:\n",
    "                    value = nested_dict.get('value', None)\n",
    "                    \n",
    "                    if source in requested_nested_list:\n",
    "\n",
    "                        source_formatted = format_source(source)\n",
    "                        key = f'{nested_field}_{source_formatted}'\n",
    "\n",
    "                        result_to_keep[key] = value\n",
    "                        \n",
    "            requested_sources = requested_nested_fields[nested_field]\n",
    "            for requested_source in requested_sources:\n",
    "                source_formatted = format_source(requested_source)\n",
    "                key = f'{nested_field}_{source_formatted}'\n",
    "                if not key in result_to_keep:\n",
    "                    result_to_keep[key] = None\n",
    "                    \n",
    "        else:\n",
    "            requested_sources = requested_nested_fields[nested_field]\n",
    "            for requested_source in requested_sources:\n",
    "                source_formatted = format_source(requested_source)\n",
    "                key = f'{nested_field}_{source_formatted}'\n",
    "                result_to_keep[key] = None\n",
    "                \n",
    "            \n",
    "\n",
    "\n",
    "    #print(result_to_keep.keys(), result_to_keep.values())\n",
    "    return t.Row(*list(result_to_keep.keys()))(*list(result_to_keep.values()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "opusdata_omdb = opusdata.withColumn(\n",
    "    \"omdb_data\", F.explode(F.array(omdb_data(F.array(\"movie_name\", \"production_year\"))))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "opusdata_fields_name = [field.name for field in opusdata.schema.fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "opusdata_ombd = opusdata_omdb.select(*opusdata_fields_name, 'omdb_data.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "opusdata_ombd_id_not_null = opusdata_ombd.na.drop(\n",
    "    subset=[\n",
    "        \"imdb_id\",\n",
    "        \"ratings_internet_movie_database\",\n",
    "        \"ratings_rotten_tomatoes\",\n",
    "        \"ratings_metacritic\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "opusdata_ombd_no_id_duplicated = opusdata_ombd_id_not_null.dropDuplicates(['imdb_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing awards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=t.IntegerType())\n",
    "def general_awards_by_keyword(awards_str, keyword):\n",
    "    n_nominations = awards_str.split(keyword)[0].split()[-1]\n",
    "    try:\n",
    "        n_nominations_int = int(n_nominations)\n",
    "    except ValueError as e:\n",
    "        n_nominations_int = 0\n",
    "    return n_nominations_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_general = ['nomination', 'win']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "awards_name = ['golden globe', 'oscar', 'bafta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=t.IntegerType())\n",
    "def won_by_keyword(awards_str, award_name):\n",
    "    awards_str = awards_str.lower()\n",
    "    \n",
    "    try:\n",
    "        won_or_nominated = awards_str.split(award_name)[0].split()[-2]\n",
    "        if won_or_nominated == \"won\":\n",
    "            n_won = int(awards_str.split(award_name)[0].split()[-1])\n",
    "        else:\n",
    "            n_won = 0\n",
    "    except IndexError as e:\n",
    "        n_won = 0\n",
    "\n",
    "    return n_won"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=t.IntegerType())\n",
    "def nominated_by_keyword(awards_str, award_name):\n",
    "    awards_str = awards_str.lower()\n",
    "   \n",
    "    try:\n",
    "        won_or_nominated = awards_str.split(award_name)[0].split()[-2]\n",
    "        if won_or_nominated == \"for\":\n",
    "            n_nominated = int(awards_str.split(award_name)[0].split()[-1])\n",
    "        else:\n",
    "            n_nominated = 0\n",
    "    except IndexError as e:\n",
    "        n_nominated = 0\n",
    "\n",
    "    return n_nominated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "opusdata_awards_categorized = opusdata_ombd_no_id_duplicated\n",
    "for general_keyword in keywords_general:\n",
    "    opusdata_awards_categorized = opusdata_awards_categorized.withColumn(\n",
    "        f'{general_keyword}s', general_awards_by_keyword(\"awards\", F.lit(general_keyword))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for award_name in awards_name:\n",
    "    award_name_formatted = '_'.join(award_name.split())\n",
    "    opusdata_awards_categorized = opusdata_awards_categorized.withColumn(\n",
    "        f'won_{award_name_formatted}s', won_by_keyword(\"awards\", F.lit(award_name))\n",
    "    )\n",
    "    opusdata_awards_categorized = opusdata_awards_categorized.withColumn(\n",
    "        f'nominated_{award_name_formatted}s', nominated_by_keyword(\"awards\", F.lit(award_name))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "opusdata_awards_categorized = opusdata_awards_categorized.drop('awards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o170.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 8, 10.0.0.8, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/utente/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/utente/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/utente/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 352, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/home/utente/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 142, in dump_stream\n    for obj in iterator:\n  File \"/home/utente/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 341, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/home/utente/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 83, in <lambda>\n    return lambda *a: toInternal(f(*a))\n  File \"/home/utente/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-15-ea11a2bdc57d>\", line 6, in omdb_data\n  File \"/home/utente/anaconda3/envs/bigdata/lib/python3.7/site-packages/omdb/client.py\", line 106, in get\n    data = self.request(timeout=timeout, **params).json()\n  File \"/home/utente/anaconda3/envs/bigdata/lib/python3.7/site-packages/omdb/client.py\", line 55, in request\n    res.raise_for_status()\n  File \"/home/utente/anaconda3/envs/bigdata/lib/python3.7/site-packages/requests/models.py\", line 941, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: http://www.omdbapi.com/?t=Safe&y=2011&page=1&plot=full&tomatoes=true&apikey=b8b4476d\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$13.apply(RDD.scala:845)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$13.apply(RDD.scala:845)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/utente/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/utente/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/utente/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 352, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/home/utente/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 142, in dump_stream\n    for obj in iterator:\n  File \"/home/utente/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 341, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/home/utente/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 83, in <lambda>\n    return lambda *a: toInternal(f(*a))\n  File \"/home/utente/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-15-ea11a2bdc57d>\", line 6, in omdb_data\n  File \"/home/utente/anaconda3/envs/bigdata/lib/python3.7/site-packages/omdb/client.py\", line 106, in get\n    data = self.request(timeout=timeout, **params).json()\n  File \"/home/utente/anaconda3/envs/bigdata/lib/python3.7/site-packages/omdb/client.py\", line 55, in request\n    res.raise_for_status()\n  File \"/home/utente/anaconda3/envs/bigdata/lib/python3.7/site-packages/requests/models.py\", line 941, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: http://www.omdbapi.com/?t=Safe&y=2011&page=1&plot=full&tomatoes=true&apikey=b8b4476d\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$13.apply(RDD.scala:845)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$13.apply(RDD.scala:845)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-388df8b84a2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mopusdata_awards_categorized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/bigdata/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \"\"\"\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bigdata/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bigdata/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bigdata/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o170.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 8, 10.0.0.8, executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/utente/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/utente/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/utente/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 352, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/home/utente/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 142, in dump_stream\n    for obj in iterator:\n  File \"/home/utente/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 341, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/home/utente/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 83, in <lambda>\n    return lambda *a: toInternal(f(*a))\n  File \"/home/utente/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-15-ea11a2bdc57d>\", line 6, in omdb_data\n  File \"/home/utente/anaconda3/envs/bigdata/lib/python3.7/site-packages/omdb/client.py\", line 106, in get\n    data = self.request(timeout=timeout, **params).json()\n  File \"/home/utente/anaconda3/envs/bigdata/lib/python3.7/site-packages/omdb/client.py\", line 55, in request\n    res.raise_for_status()\n  File \"/home/utente/anaconda3/envs/bigdata/lib/python3.7/site-packages/requests/models.py\", line 941, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: http://www.omdbapi.com/?t=Safe&y=2011&page=1&plot=full&tomatoes=true&apikey=b8b4476d\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$13.apply(RDD.scala:845)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$13.apply(RDD.scala:845)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/utente/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/home/utente/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/utente/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 352, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/home/utente/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 142, in dump_stream\n    for obj in iterator:\n  File \"/home/utente/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 341, in _batched\n    for item in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/home/utente/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 83, in <lambda>\n    return lambda *a: toInternal(f(*a))\n  File \"/home/utente/spark-2.4.5-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-15-ea11a2bdc57d>\", line 6, in omdb_data\n  File \"/home/utente/anaconda3/envs/bigdata/lib/python3.7/site-packages/omdb/client.py\", line 106, in get\n    data = self.request(timeout=timeout, **params).json()\n  File \"/home/utente/anaconda3/envs/bigdata/lib/python3.7/site-packages/omdb/client.py\", line 55, in request\n    res.raise_for_status()\n  File \"/home/utente/anaconda3/envs/bigdata/lib/python3.7/site-packages/requests/models.py\", line 941, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: http://www.omdbapi.com/?t=Safe&y=2011&page=1&plot=full&tomatoes=true&apikey=b8b4476d\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$JoinIterator.hasNext(Iterator.scala:212)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:80)\n\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:77)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$13.apply(RDD.scala:845)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndexInternal$1$$anonfun$13.apply(RDD.scala:845)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "opusdata_awards_categorized.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale rankings [0..1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale imdb ratings\n",
    "opusdata_scaled_ratings = opusdata_awards_categorized.withColumn(\n",
    "    \"ratings_internet_movie_database\", F.split(F.col(\"ratings_internet_movie_database\"), \"/\").cast(\"array<float>\") \\\n",
    "   \n",
    ")\n",
    "opusdata_scaled_ratings = opusdata_scaled_ratings.withColumn(\n",
    "    \"ratings_internet_movie_database\", F.col(\"ratings_internet_movie_database\")[0] / 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale rotten tomatoes ratings\n",
    "opusdata_scaled_ratings = opusdata_scaled_ratings.withColumn(\n",
    "    \"ratings_rotten_tomatoes\", F.split(F.col(\"ratings_rotten_tomatoes\"), \"%\").cast(\"array<int>\") \\\n",
    "   \n",
    ")\n",
    "opusdata_scaled_ratings = opusdata_scaled_ratings.withColumn(\n",
    "    \"ratings_rotten_tomatoes\", F.col(\"ratings_rotten_tomatoes\")[0] / 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale metacritic ratings\n",
    "opusdata_scaled_ratings = opusdata_scaled_ratings.withColumn(\n",
    "    \"ratings_metacritic\", F.split(F.col(\"ratings_metacritic\"), \"/\").cast(\"array<int>\") \\\n",
    "   \n",
    ")\n",
    "opusdata_scaled_ratings = opusdata_scaled_ratings.withColumn(\n",
    "    \"ratings_metacritic\", F.col(\"ratings_metacritic\")[0] / 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove comma from imdb_votes\n",
    "opusdata_votes = opusdata_scaled_ratings.withColumn(\n",
    "    \"imdb_votes\", F.regexp_replace(\"imdb_votes\", \",\", \"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode actors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_actors = set()\n",
    "\n",
    "for i, row in enumerate(opusdata_votes.rdd.collect()):\n",
    "    actors = row['actors']\n",
    "    unique_actors.update([a.strip().lower() for a in actors.split(',')])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actors_id_dict = {actor: i for i, actor in enumerate(unique_actors)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_actors = t.StructType([\n",
    "    t.StructField('actor_id_0', t.IntegerType(), True),\n",
    "    t.StructField('actor_id_1', t.IntegerType(), True),\n",
    "    t.StructField('actor_id_2', t.IntegerType(), True),\n",
    "    t.StructField('actor_id_3', t.IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=schema_actors)\n",
    "def encode_authors(actors_str):\n",
    "    actors = [a.strip().lower() for a in actors_str.split(',')]\n",
    "    \n",
    "    ids = []\n",
    "    for a in actors:\n",
    "        ids.append(actors_id_dict[a])\n",
    "    \n",
    "    ids = sorted(ids) + (4-len(ids))*[None]\n",
    "        \n",
    "    return t.Row('actor_id_0', 'actor_id_1', 'actor_id_2', 'actor_id_3')(*ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opusdata_actors = opusdata_votes.withColumn(\n",
    "    \"actors_ids\", F.explode(F.array(encode_authors(\"actors\")))\n",
    ")\n",
    "\n",
    "opusdata_fields_name = [\n",
    "    field.name\n",
    "    for field in opusdata_actors.schema.fields\n",
    "    if field.name != \"actors_ids\" and field.name != \"actors\"\n",
    "]\n",
    "opusdata_actors = opusdata_actors.select(*opusdata_fields_name, \"actors_ids.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime - remove \"min\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opusdata_runtime = opusdata_actors.withColumn(\n",
    "    \"runtime\", F.split(F.col(\"runtime\"), \" \").cast(\"array<string>\")\n",
    ")\n",
    "opusdata_runtime = opusdata_runtime.withColumn(\"runtime\", F.col(\"runtime\")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep only first country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opusdata_first_country = opusdata_runtime.withColumn(\n",
    "    \"country\", F.split(F.col(\"country\"), \",\").cast(\"array<string>\") \\\n",
    "   \n",
    ")\n",
    "opusdata_first_country = opusdata_first_country.withColumn(\n",
    "    \"country\", F.col(\"country\")[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep only first director"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opusdata_first_director = opusdata_first_country.withColumn(\n",
    "    \"director\", F.split(F.col(\"director\"), \",\").cast(\"array<string>\") \\\n",
    "   \n",
    ")\n",
    "opusdata_first_director = opusdata_first_director.withColumn(\n",
    "    \"director\", F.col(\"director\")[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opusdata_first_director.repartition(1).write.mode(\"overwrite\").option('header',True).csv(\n",
    "    f\"hdfs://{HADOOP_NAMENODE}:8020/processed/opusdata_omdb.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bigdata)",
   "language": "python",
   "name": "bigdata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
